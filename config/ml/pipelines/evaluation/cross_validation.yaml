# Cross Validation Pipeline Configuration
# ======================================

pipeline:
  name: "cross_validation_pipeline"
  version: "1.0.0"
  description: "Cross-validation pipeline for model performance assessment"
  
  # Pipeline metadata
  metadata:
    author: "ML Team"
    created_date: "2024-01-01"
    last_updated: "2024-01-01"
    tags: ["cross_validation", "model_evaluation", "performance_assessment", "validation"]

# Pipeline Stages
stages:
  - name: "data_preparation"
    type: "data_preparator"
    description: "Prepare datasets for cross-validation"
    config:
      datasets:
        - name: "training_set"
          source: "postgresql"
          table: "training_data"
          filters:
            date_range: "last_90_days"
            quality_threshold: 0.95
        - name: "validation_set"
          source: "postgresql"
          table: "validation_data"
          filters:
            date_range: "last_30_days"
            quality_threshold: 0.98
        - name: "test_set"
          source: "postgresql"
          table: "test_data"
          filters:
            date_range: "last_7_days"
            quality_threshold: 0.99
      preprocessing:
        normalization: true
        feature_engineering: true
        data_cleaning: true
      output: "prepared_datasets"
      
  - name: "model_loading"
    type: "model_loader"
    description: "Load models for cross-validation"
    config:
      models:
        - name: "defect_detector"
          registry: "mlflow"
          stage: "production"
          version: "latest"
          type: "classification"
        - name: "quality_assessor"
          registry: "mlflow"
          stage: "production"
          version: "latest"
          type: "regression"
        - name: "process_optimizer"
          registry: "mlflow"
          stage: "production"
          version: "latest"
          type: "multi_objective_optimization"
        - name: "equipment_health_monitor"
          registry: "mlflow"
          stage: "production"
          version: "latest"
          type: "anomaly_detection"
      loading_strategy: "parallel"
      caching: true
      output: "loaded_models"
      
  - name: "cross_validation_setup"
    type: "cv_setup"
    description: "Setup cross-validation configuration"
    config:
      cv_strategies:
        - name: "k_fold"
          enabled: true
          folds: 5
          shuffle: true
          random_state: 42
        - name: "stratified_k_fold"
          enabled: true
          folds: 5
          shuffle: true
          random_state: 42
        - name: "time_series_split"
          enabled: true
          n_splits: 5
          test_size: 0.2
        - name: "group_k_fold"
          enabled: true
          folds: 5
          group_column: "process_id"
      validation_config:
        scoring_metrics:
          classification:
            - "accuracy"
            - "precision_macro"
            - "recall_macro"
            - "f1_macro"
            - "roc_auc"
            - "average_precision"
          regression:
            - "r2_score"
            - "mse"
            - "mae"
            - "rmse"
            - "mape"
            - "explained_variance"
          multi_objective:
            - "hypervolume"
            - "generational_distance"
            - "inverted_generational_distance"
            - "spread"
            - "epsilon_indicator"
          anomaly_detection:
            - "roc_auc"
            - "average_precision"
            - "f1_score"
            - "precision"
            - "recall"
      output: "cv_configuration"
      
  - name: "cross_validation_execution"
    type: "cv_executor"
    description: "Execute cross-validation"
    config:
      execution_strategy: "parallel"
      max_workers: 4
      timeout: "2h"
      cv_configuration: "{{ cv_configuration }}"
      models: "{{ loaded_models }}"
      datasets: "{{ prepared_datasets }}"
      output: "cv_results"
      
  - name: "performance_analysis"
    type: "performance_analyzer"
    description: "Analyze cross-validation performance"
    config:
      analysis_metrics:
        - "mean_score"
        - "std_score"
        - "min_score"
        - "max_score"
        - "confidence_interval"
        - "statistical_significance"
      statistical_tests:
        - type: "t_test"
          alpha: 0.05
          alternative: "two-sided"
        - type: "wilcoxon"
          alpha: 0.05
          alternative: "two-sided"
        - type: "mann_whitney"
          alpha: 0.05
          alternative: "two-sided"
      visualization:
        enabled: true
        plots:
          - "box_plot"
          - "violin_plot"
          - "distribution_plot"
          - "learning_curve"
          - "validation_curve"
      output: "performance_analysis"
      
  - name: "model_comparison"
    type: "model_comparator"
    description: "Compare model performance"
    config:
      comparison_metrics:
        - "performance"
        - "stability"
        - "robustness"
        - "efficiency"
      ranking_method:
        type: "weighted_sum"
        weights:
          performance: 0.4
          stability: 0.3
          robustness: 0.2
          efficiency: 0.1
      statistical_tests:
        - type: "friedman_test"
          alpha: 0.05
        - type: "nemenyi_test"
          alpha: 0.05
      output: "model_comparison"
      
  - name: "report_generation"
    type: "report_generator"
    description: "Generate cross-validation report"
    config:
      report_types:
        - "executive_summary"
        - "technical_report"
        - "visualization_dashboard"
        - "model_comparison"
      formats:
        - "html"
        - "pdf"
        - "json"
        - "csv"
      visualizations:
        - type: "box_plot"
        - type: "violin_plot"
        - type: "distribution_plot"
        - type: "learning_curve"
        - type: "validation_curve"
        - type: "performance_comparison"
        - type: "statistical_significance"
      output: "cv_report"

# Pipeline Scheduling
scheduling:
  trigger: "cron"
  schedule: "0 4 * * 0"  # Weekly on Sunday at 4 AM
  timezone: "UTC"
  enabled: true
  
  # Retry configuration
  retry_policy:
    max_retries: 2
    backoff_factor: 2
    retry_delay: 1800  # 30 minutes

# Resource Requirements
resources:
  cpu: "16"
  memory: "32Gi"
  gpu: "4"
  storage: "200Gi"
  
  # Resource limits
  limits:
    cpu: "32"
    memory: "64Gi"
    gpu: "8"
    storage: "500Gi"

# Environment Configuration
environment:
  python_version: "3.11"
  dependencies:
    - "tensorflow==2.16.1"
    - "scikit-learn==1.3.0"
    - "pandas==2.0.3"
    - "numpy==1.24.3"
    - "mlflow==2.9.2"
    - "matplotlib==3.7.2"
    - "seaborn==0.12.2"
    - "plotly==5.15.0"
    - "scipy==1.11.4"
    - "statsmodels==0.14.0"
  
  # Environment variables
  env_vars:
    MLFLOW_TRACKING_URI: "http://mlflow:5000"
    PYTHONPATH: "/app/src:/app/config"

# Error Handling
error_handling:
  # Retry policy
  retry_policy:
    max_retries: 2
    backoff_factor: 2
    retry_delay: 1800
  
  # Failure handling
  failure_handling:
    strategy: "stop_on_failure"
    cleanup_on_failure: true
    preserve_logs: true
  
  # Notification
  notification:
    channels: ["email", "slack"]
    recipients: ["ml-team@company.com"]
    on_success: true
    on_failure: true

# Monitoring and Logging
monitoring:
  # Pipeline monitoring
  pipeline_monitoring:
    enabled: true
    metrics:
      - "execution_time"
      - "memory_usage"
      - "cpu_usage"
      - "stage_duration"
      - "cv_fold_duration"
    
  # Cross-validation monitoring
  cv_monitoring:
    enabled: true
    metrics:
      - "cv_score_mean"
      - "cv_score_std"
      - "cv_score_min"
      - "cv_score_max"
      - "cv_convergence_rate"
      - "cv_failure_rate"
    
  # Resource monitoring
  resource_monitoring:
    enabled: true
    metrics:
      - "gpu_utilization"
      - "memory_utilization"
      - "cpu_utilization"
      - "storage_usage"

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    - type: "console"
    - type: "file"
      filename: "/app/logs/cross_validation.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
  
  # Stage-specific logging
  stage_logging:
    enabled: true
    log_inputs: true
    log_outputs: true
    log_metrics: true
    log_artifacts: true

# Artifact Management
artifacts:
  # Artifact storage
  storage:
    type: "s3"
    bucket: "pbf-ml-artifacts"
    path: "cross_validation/reports"
    compression: "gzip"
  
  # Artifact retention
  retention:
    enabled: true
    days: 90
    keep_best: 10
    keep_latest: 20
  
  # Artifact types
  types:
    - "cv_report"
    - "performance_metrics"
    - "model_comparison"
    - "visualizations"
    - "statistical_tests"
    - "learning_curves"
    - "validation_curves"

# Pipeline Dependencies
dependencies:
  # Data dependencies
  data_dependencies:
    - source: "postgresql"
      table: "training_data"
      min_rows: 10000
    - source: "postgresql"
      table: "validation_data"
      min_rows: 2000
    - source: "postgresql"
      table: "test_data"
      min_rows: 1000
  
  # Model dependencies
  model_dependencies:
    - type: "model_registry"
      name: "defect_detector"
      stage: "production"
    - type: "model_registry"
      name: "quality_assessor"
      stage: "production"
    - type: "model_registry"
      name: "process_optimizer"
      stage: "production"
    - type: "model_registry"
      name: "equipment_health_monitor"
      stage: "production"
  
  # Infrastructure dependencies
  infrastructure_dependencies:
    - service: "mlflow"
      endpoint: "http://mlflow:5000"
    - service: "postgresql"
      endpoint: "postgres:5432"
    - service: "s3"
      endpoint: "s3://pbf-ml-artifacts"
