# Benchmark Testing Pipeline Configuration
# Pipeline for comprehensive model benchmarking and testing

pipeline:
  name: "benchmark_testing_pipeline"
  version: "1.0.0"
  description: "Comprehensive model benchmarking and testing pipeline"
  
  stages:
    - name: "data_ingestion"
      description: "Ingest benchmark datasets and model artifacts"
      component: "data_ingestion"
      inputs:
        - source: "benchmark_datasets"
          path: "/data/benchmarks/"
          format: "parquet"
        - source: "model_artifacts"
          path: "/models/registry/"
          format: "mlflow"
        - source: "baseline_models"
          path: "/models/baselines/"
          format: "pickle"
        - source: "test_scenarios"
          path: "/data/test_scenarios/"
          format: "json"
      outputs:
        - name: "benchmark_data"
          path: "/data/pipeline/benchmark_testing/input/"
          format: "parquet"
      parameters:
        batch_size: 5000
        parallel_workers: 4
        timeout_minutes: 60
        
    - name: "test_scenario_preparation"
      description: "Prepare test scenarios and datasets"
      component: "test_scenario_preparation"
      inputs:
        - name: "benchmark_data"
          from_stage: "data_ingestion"
      outputs:
        - name: "test_scenarios"
          path: "/data/pipeline/benchmark_testing/scenarios/"
          format: "parquet"
      parameters:
        scenarios:
          - name: "normal_operation"
            description: "Normal operating conditions"
            data_filter: "status == 'normal'"
            sample_size: 10000
          - name: "edge_cases"
            description: "Edge cases and boundary conditions"
            data_filter: "edge_case == True"
            sample_size: 1000
          - name: "adversarial_samples"
            description: "Adversarial and challenging samples"
            data_filter: "adversarial == True"
            sample_size: 500
          - name: "data_drift"
            description: "Data drift scenarios"
            data_filter: "drift_detected == True"
            sample_size: 2000
          - name: "high_load"
            description: "High load and stress conditions"
            data_filter: "load_factor > 0.8"
            sample_size: 1500
          - name: "low_quality_data"
            description: "Low quality and noisy data"
            data_filter: "data_quality < 0.7"
            sample_size: 1000
        data_augmentation:
          enabled: true
          techniques:
            - "noise_injection"
            - "feature_corruption"
            - "missing_value_injection"
            - "outlier_injection"
        validation:
          enabled: true
          completeness_check: true
          distribution_check: true
          
    - name: "model_loading"
      description: "Load models for benchmarking"
      component: "model_loading"
      inputs:
        - name: "benchmark_data"
          from_stage: "data_ingestion"
      outputs:
        - name: "loaded_models"
          path: "/data/pipeline/benchmark_testing/models/"
          format: "pickle"
      parameters:
        models:
          - name: "current_model"
            type: "production"
            version: "latest"
            path: "/models/production/"
          - name: "baseline_model"
            type: "baseline"
            version: "v1.0"
            path: "/models/baselines/"
          - name: "competitor_model"
            type: "external"
            version: "external_v1"
            path: "/models/external/"
          - name: "ensemble_model"
            type: "ensemble"
            version: "ensemble_v1"
            path: "/models/ensemble/"
        model_validation:
          enabled: true
          schema_validation: true
          version_validation: true
          dependency_check: true
        fallback_strategy: "skip_failed_models"
        
    - name: "performance_benchmarking"
      description: "Run comprehensive performance benchmarks"
      component: "performance_benchmarking"
      inputs:
        - name: "test_scenarios"
          from_stage: "test_scenario_preparation"
        - name: "loaded_models"
          from_stage: "model_loading"
      outputs:
        - name: "benchmark_results"
          path: "/data/pipeline/benchmark_testing/results/"
          format: "json"
      parameters:
        benchmarks:
          - name: "accuracy_benchmark"
            metrics:
              - "accuracy"
              - "precision"
              - "recall"
              - "f1_score"
              - "auc_roc"
            scenarios: ["normal_operation", "edge_cases", "adversarial_samples"]
          - name: "speed_benchmark"
            metrics:
              - "inference_time"
              - "throughput"
              - "latency_p95"
              - "latency_p99"
            scenarios: ["normal_operation", "high_load"]
          - name: "robustness_benchmark"
            metrics:
              - "robustness_score"
              - "adversarial_accuracy"
              - "noise_tolerance"
              - "outlier_resistance"
            scenarios: ["adversarial_samples", "low_quality_data"]
          - name: "scalability_benchmark"
            metrics:
              - "memory_usage"
              - "cpu_utilization"
              - "gpu_utilization"
              - "scaling_efficiency"
            scenarios: ["high_load"]
          - name: "fairness_benchmark"
            metrics:
              - "demographic_parity"
              - "equalized_odds"
              - "equal_opportunity"
              - "calibration"
            scenarios: ["normal_operation"]
        statistical_tests:
          enabled: true
          tests:
            - "t_test"
            - "wilcoxon_test"
            - "mann_whitney_u"
            - "ks_test"
        confidence_intervals:
          enabled: true
          confidence_level: 0.95
          method: "bootstrap"
          n_bootstrap: 1000
          
    - name: "stress_testing"
      description: "Perform stress testing on models"
      component: "stress_testing"
      inputs:
        - name: "loaded_models"
          from_stage: "model_loading"
        - name: "test_scenarios"
          from_stage: "test_scenario_preparation"
      outputs:
        - name: "stress_test_results"
          path: "/data/pipeline/benchmark_testing/stress_test/"
          format: "json"
      parameters:
        stress_tests:
          - name: "load_testing"
            parameters:
              max_concurrent_requests: 1000
              ramp_up_time: "5_minutes"
              duration: "30_minutes"
              success_rate_threshold: 0.95
          - name: "memory_stress"
            parameters:
              memory_limit: "8GB"
              memory_pressure: "gradual"
              duration: "20_minutes"
          - name: "cpu_stress"
            parameters:
              cpu_limit: "80%"
              cpu_pressure: "sustained"
              duration: "15_minutes"
          - name: "data_volume_stress"
            parameters:
              max_data_size: "10GB"
              data_growth_rate: "exponential"
              duration: "25_minutes"
          - name: "network_stress"
            parameters:
              bandwidth_limit: "100Mbps"
              latency_injection: "100ms"
              packet_loss: "0.1%"
              duration: "20_minutes"
        failure_detection:
          enabled: true
          metrics:
            - "response_time_degradation"
            - "error_rate_increase"
            - "resource_exhaustion"
            - "memory_leak_detection"
        recovery_testing:
          enabled: true
          scenarios:
            - "graceful_degradation"
            - "failover_recovery"
            - "auto_scaling"
            
    - name: "adversarial_testing"
      description: "Perform adversarial testing on models"
      component: "adversarial_testing"
      inputs:
        - name: "loaded_models"
          from_stage: "model_loading"
        - name: "test_scenarios"
          from_stage: "test_scenario_preparation"
      outputs:
        - name: "adversarial_test_results"
          path: "/data/pipeline/benchmark_testing/adversarial/"
          format: "json"
      parameters:
        adversarial_attacks:
          - name: "fgsm_attack"
            parameters:
              epsilon: [0.01, 0.05, 0.1, 0.2]
              targeted: false
          - name: "pgd_attack"
            parameters:
              epsilon: [0.01, 0.05, 0.1]
              iterations: 10
              step_size: 0.01
          - name: "carlini_wagner"
            parameters:
              confidence: 0
              learning_rate: 0.01
              max_iterations: 1000
          - name: "deepfool"
            parameters:
              max_iterations: 50
              overshoot: 0.02
          - name: "feature_perturbation"
            parameters:
              perturbation_ratio: [0.1, 0.2, 0.3]
              feature_types: ["numerical", "categorical"]
        robustness_metrics:
          - "adversarial_accuracy"
          - "robustness_radius"
          - "attack_success_rate"
          - "perturbation_magnitude"
        defense_evaluation:
          enabled: true
          methods:
            - "adversarial_training"
            - "input_preprocessing"
            - "ensemble_methods"
            - "certified_defenses"
            
    - name: "comparative_analysis"
      description: "Compare models across all benchmarks"
      component: "comparative_analysis"
      inputs:
        - name: "benchmark_results"
          from_stage: "performance_benchmarking"
        - name: "stress_test_results"
          from_stage: "stress_testing"
        - name: "adversarial_test_results"
          from_stage: "adversarial_testing"
      outputs:
        - name: "comparative_analysis"
          path: "/data/pipeline/benchmark_testing/comparative/"
          format: "json"
      parameters:
        comparison_dimensions:
          - "accuracy"
          - "speed"
          - "robustness"
          - "scalability"
          - "fairness"
          - "resource_usage"
        ranking_methods:
          - "weighted_score"
          - "pareto_optimality"
          - "statistical_significance"
          - "business_impact"
        statistical_tests:
          enabled: true
          tests:
            - "anova"
            - "kruskal_wallis"
            - "friedman_test"
        visualization:
          enabled: true
          plots:
            - "radar_chart"
            - "performance_matrix"
            - "statistical_significance"
            - "ranking_visualization"
            
    - name: "report_generation"
      description: "Generate comprehensive benchmark report"
      component: "report_generation"
      inputs:
        - name: "benchmark_results"
          from_stage: "performance_benchmarking"
        - name: "stress_test_results"
          from_stage: "stress_testing"
        - name: "adversarial_test_results"
          from_stage: "adversarial_testing"
        - name: "comparative_analysis"
          from_stage: "comparative_analysis"
      outputs:
        - name: "benchmark_report"
          path: "/data/pipeline/benchmark_testing/reports/"
          format: "html"
      parameters:
        report_sections:
          - "executive_summary"
          - "performance_benchmarks"
          - "stress_test_results"
          - "adversarial_test_results"
          - "comparative_analysis"
          - "recommendations"
          - "appendix"
        visualization:
          enabled: true
          plots:
            - "performance_dashboard"
            - "stress_test_plots"
            - "adversarial_robustness"
            - "comparative_charts"
            - "ranking_visualization"
        export_formats:
          - "html"
          - "pdf"
          - "json"
          - "csv"
        template: "benchmark_report_template.html"
        
    - name: "pipeline_monitoring"
      description: "Monitor benchmark testing pipeline"
      component: "pipeline_monitoring"
      inputs:
        - name: "all_stages"
          from_stage: "*"
      outputs:
        - name: "monitoring_metrics"
          path: "/data/pipeline/benchmark_testing/monitoring/"
          format: "json"
      parameters:
        metrics:
          - "pipeline_execution_time"
          - "benchmark_completion_rate"
          - "test_success_rate"
          - "resource_utilization"
          - "report_generation_time"
        alerting:
          - name: "pipeline_failure"
            condition: "status == 'failed'"
            severity: "critical"
          - name: "benchmark_failure"
            condition: "benchmark_success_rate < 0.9"
            severity: "high"
          - name: "stress_test_failure"
            condition: "stress_test_failure_rate > 0.1"
            severity: "warning"
        notifications:
          channels: ["email", "slack", "dashboard"]
          recipients: ["ml_team", "qa_team", "engineering_team"]

  scheduling:
    trigger: "schedule"
    schedule: "0 2 * * 6"  # Every Saturday at 2 AM
    timezone: "UTC"
    retry_policy:
      max_retries: 3
      retry_delay_minutes: 60
      exponential_backoff: true
      
  resources:
    compute:
      type: "spark_cluster"
      nodes: 8
      cores_per_node: 16
      memory_per_node: "128GB"
      storage: "1TB"
    gpu:
      enabled: true
      type: "nvidia_tesla_v100"
      count: 4
      memory_per_gpu: "32GB"
    storage:
      type: "s3"
      bucket: "ml-benchmark-testing"
      region: "us-west-2"
      
  environment:
    python_version: "3.9"
    dependencies:
      - "scikit-learn==1.3.0"
      - "tensorflow==2.13.0"
      - "torch==2.0.1"
      - "adversarial-robustness-toolbox==1.15.0"
      - "pandas==2.0.3"
      - "numpy==1.24.3"
      - "matplotlib==3.7.2"
      - "seaborn==0.12.2"
      - "plotly==5.15.0"
      - "mlflow==2.5.0"
      - "fairlearn==0.9.0"
      - "scipy==1.11.1"
      - "statsmodels==0.14.0"
    environment_variables:
      MLFLOW_TRACKING_URI: "http://mlflow-server:5000"
      AWS_DEFAULT_REGION: "us-west-2"
      LOG_LEVEL: "INFO"
      CUDA_VISIBLE_DEVICES: "0,1,2,3"
      
  monitoring:
    enabled: true
    metrics_collection:
      - "pipeline_execution_time"
      - "benchmark_completion_rate"
      - "test_success_rate"
      - "resource_utilization"
      - "report_generation_time"
    logging:
      level: "INFO"
      format: "json"
      retention_days: 60
    alerting:
      enabled: true
      channels: ["email", "slack"]
      thresholds:
        execution_time_hours: 8
        failure_rate_percent: 10
        benchmark_success_rate: 0.9
