# Batch Predictor Service Configuration
# =====================================

service:
  name: "batch_predictor_service"
  version: "1.0.0"
  description: "Batch prediction service for large-scale ML inference"
  
  # Service metadata
  metadata:
    author: "ML Team"
    created_date: "2024-01-01"
    last_updated: "2024-01-01"
    tags: ["batch_prediction", "large_scale", "inference", "scheduled"]

# Service Endpoints
endpoints:
  - name: "submit_batch_job"
    path: "/api/v1/batch/submit"
    method: "POST"
    description: "Submit a batch prediction job"
    
    # Input schema
    input_schema:
      type: "object"
      properties:
        job_name:
          type: "string"
          description: "Unique name for the batch job"
        data_source:
          type: "object"
          properties:
            type: 
              type: "string"
              enum: ["s3", "postgresql", "mongodb", "kafka"]
            location: 
              type: "string"
              description: "Location of the data source"
            format: 
              type: "string"
              enum: ["parquet", "csv", "json", "avro"]
            schema: 
              type: "object"
              description: "Data schema definition"
          required: ["type", "location", "format"]
        model_config:
          type: "object"
          properties:
            model_name: 
              type: "string"
              description: "Name of the model to use"
            model_version: 
              type: "string"
              description: "Version of the model"
            model_stage: 
              type: "string"
              enum: ["staging", "production"]
              description: "Stage of the model"
          required: ["model_name", "model_version", "model_stage"]
        output_config:
          type: "object"
          properties:
            destination: 
              type: "string"
              description: "Output destination"
            format: 
              type: "string"
              enum: ["parquet", "csv", "json", "avro"]
            partitioning: 
              type: "array"
              items: {"type": "string"}
              description: "Partitioning columns"
            compression: 
              type: "string"
              enum: ["none", "gzip", "snappy", "lz4"]
          required: ["destination", "format"]
        processing_config:
          type: "object"
          properties:
            batch_size: 
              type: "integer"
              minimum: 1
              maximum: 10000
              default: 1000
            parallelism: 
              type: "integer"
              minimum: 1
              maximum: 100
              default: 4
            memory_per_worker: 
              type: "string"
              default: "2Gi"
            cpu_per_worker: 
              type: "string"
              default: "1"
          required: ["batch_size", "parallelism"]
        scheduling:
          type: "object"
          properties:
            schedule: 
              type: "string"
              description: "Cron expression for scheduling"
            timezone: 
              type: "string"
              default: "UTC"
            enabled: 
              type: "boolean"
              default: true
      required: ["job_name", "data_source", "model_config", "output_config"]
    
    # Output schema
    output_schema:
      type: "object"
      properties:
        job_id: 
          type: "string"
          description: "Unique identifier for the batch job"
        status: 
          type: "string"
          enum: ["submitted", "running", "completed", "failed", "cancelled"]
        submitted_at: 
          type: "string"
          format: "date-time"
        estimated_completion: 
          type: "string"
          format: "date-time"
        progress: 
          type: "object"
          properties:
            total_records: {"type": "integer"}
            processed_records: {"type": "integer"}
            percentage: {"type": "number", "minimum": 0, "maximum": 100}
        output_location: 
          type: "string"
          description: "Location of the output results"
        metadata: 
          type: "object"
          description: "Additional job metadata"

  - name: "get_job_status"
    path: "/api/v1/batch/status/{job_id}"
    method: "GET"
    description: "Get the status of a batch prediction job"
    
    # Output schema
    output_schema:
      type: "object"
      properties:
        job_id: 
          type: "string"
        status: 
          type: "string"
          enum: ["submitted", "running", "completed", "failed", "cancelled"]
        progress: 
          type: "object"
          properties:
            total_records: {"type": "integer"}
            processed_records: {"type": "integer"}
            percentage: {"type": "number", "minimum": 0, "maximum": 100}
            current_stage: {"type": "string"}
            estimated_remaining_time: {"type": "string"}
        submitted_at: 
          type: "string"
          format: "date-time"
        started_at: 
          type: "string"
          format: "date-time"
        completed_at: 
          type: "string"
          format: "date-time"
        error_message: 
          type: "string"
        output_location: 
          type: "string"
        metrics: 
          type: "object"
          properties:
            processing_time: {"type": "number"}
            throughput: {"type": "number"}
            memory_usage: {"type": "number"}
            cpu_usage: {"type": "number"}

  - name: "cancel_job"
    path: "/api/v1/batch/cancel/{job_id}"
    method: "POST"
    description: "Cancel a running batch prediction job"
    
    # Output schema
    output_schema:
      type: "object"
      properties:
        job_id: 
          type: "string"
        status: 
          type: "string"
          enum: ["cancelled", "cancellation_failed"]
        message: 
          type: "string"

# Batch Processing Configuration
batch_processing:
  # Processing engine
  engine: "spark"
  version: "3.5.0"
  
  # Processing stages
  stages:
    - name: "data_loading"
      description: "Load data from source"
      timeout: "1h"
    - name: "data_validation"
      description: "Validate input data"
      timeout: "30m"
    - name: "feature_engineering"
      description: "Engineer features from raw data"
      timeout: "2h"
    - name: "model_loading"
      description: "Load trained models"
      timeout: "30m"
    - name: "inference"
      description: "Run batch inference"
      timeout: "4h"
    - name: "result_processing"
      description: "Process and format results"
      timeout: "1h"
    - name: "output_writing"
      description: "Write results to destination"
      timeout: "1h"
  
  # Resource management
  resource_management:
    default_workers: 4
    max_workers: 20
    worker_memory: "4Gi"
    worker_cpu: "2"
    driver_memory: "8Gi"
    driver_cpu: "2"
    
    # Dynamic scaling
    dynamic_scaling:
      enabled: true
      scale_up_threshold: 0.8
      scale_down_threshold: 0.3
      min_workers: 2
      max_workers: 50
  
  # Data processing
  data_processing:
    # Batch size configuration
    batch_sizes:
      small: 1000
      medium: 10000
      large: 100000
      xlarge: 1000000
    
    # Memory management
    memory_management:
      spill_to_disk: true
      spill_threshold: 0.8
      compression: "snappy"
      cache_level: "MEMORY_AND_DISK"
    
    # Checkpointing
    checkpointing:
      enabled: true
      interval: "10m"
      location: "s3://pbf-checkpoints/batch-processing"

# Model Configuration
model:
  # Model loading
  loading:
    strategy: "lazy_loading"
    cache_models: true
    max_cached_models: 10
    model_refresh_interval: "2h"
  
  # Model preprocessing
  preprocessing:
    normalization: true
    scaling_method: "standard_scaler"
    feature_engineering: true
    parallel_processing: true
  
  # Model postprocessing
  postprocessing:
    confidence_threshold: 0.8
    ensemble_voting: false
    calibration: true
    output_formatting: true
    parallel_processing: true

# Service Configuration
service_config:
  # Host and port
  host: "0.0.0.0"
  port: 8001
  
  # Workers and concurrency
  workers: 2
  max_concurrent_jobs: 10
  job_timeout: "24h"  # 24 hours
  
  # Job queue
  job_queue:
    type: "redis"
    max_queue_size: 100
    priority_levels: 3
    retry_failed_jobs: true
    max_retries: 3

# Performance Configuration
performance:
  # Throughput requirements
  throughput:
    target: 10000  # records per second
    max: 100000
    burst_capacity: 50000
  
  # Latency requirements
  latency:
    job_submission: 5  # seconds
    status_check: 1  # seconds
    result_retrieval: 10  # seconds
  
  # Memory requirements
  memory:
    target: 8  # GB
    max: 32
    gc_threshold: 0.8
  
  # CPU requirements
  cpu:
    target: 4
    max: 16
    affinity: "auto"

# Monitoring Configuration
monitoring:
  # Health checks
  health_check:
    path: "/health"
    interval: "60s"
    timeout: "10s"
    failure_threshold: 3
    success_threshold: 1
  
  # Metrics collection
  metrics:
    enabled: true
    collection_interval: "5m"
    retention_days: 90
    
    # Custom metrics
    custom_metrics:
      - "active_jobs"
      - "queued_jobs"
      - "completed_jobs"
      - "failed_jobs"
      - "average_job_duration"
      - "throughput_per_second"
      - "memory_usage"
      - "cpu_usage"
      - "disk_usage"
      - "network_io"
  
  # Alerting
  alerting:
    enabled: true
    channels: ["email", "slack", "pagerduty"]
    recipients: ["ml-team@company.com"]
    
    # Alert thresholds
    thresholds:
      - metric: "active_jobs"
        threshold: 20
        operator: ">"
        severity: "warning"
        duration: "10m"
      - metric: "failed_jobs"
        threshold: 0.1
        operator: ">"
        severity: "critical"
        duration: "5m"
      - metric: "average_job_duration"
        threshold: 3600  # 1 hour
        operator: ">"
        severity: "warning"
        duration: "30m"
      - metric: "memory_usage"
        threshold: 0.9
        operator: ">"
        severity: "critical"
        duration: "5m"

# Security Configuration
security:
  # Authentication
  authentication:
    enabled: true
    type: "jwt"
    secret_key: "${JWT_SECRET_KEY}"
    algorithm: "HS256"
    expiration_hours: 24
  
  # Authorization
  authorization:
    enabled: true
    type: "rbac"
    roles:
      - "ml_admin"
      - "ml_engineer"
      - "ml_user"
    permissions:
      ml_admin: ["read", "write", "admin", "cancel"]
      ml_engineer: ["read", "write", "cancel"]
      ml_user: ["read", "write"]
  
  # Data encryption
  encryption:
    enabled: true
    algorithm: "AES-256-GCM"
    encrypt_data_at_rest: true
    encrypt_data_in_transit: true

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  handlers:
    - type: "console"
    - type: "file"
      filename: "/app/logs/batch_predictor_service.log"
      max_bytes: 10485760  # 10MB
      backup_count: 5
  
  # Job logging
  job_logging:
    enabled: true
    log_job_submission: true
    log_job_progress: true
    log_job_completion: true
    log_job_errors: true
    log_performance_metrics: true

# Deployment Configuration
deployment:
  # Deployment strategy
  strategy: "rolling"
  replicas: 2
  max_unavailable: 1
  max_surge: 1
  progress_deadline: 1200  # 20 minutes
  
  # Resource requirements
  resources:
    requests:
      cpu: "4"
      memory: "8Gi"
    limits:
      cpu: "8"
      memory: "32Gi"
  
  # Health checks
  health_checks:
    liveness:
      enabled: true
      path: "/health"
      initial_delay: 60
      period: 30
      timeout: 10
      success_threshold: 1
      failure_threshold: 3
    readiness:
      enabled: true
      path: "/health"
      initial_delay: 30
      period: 10
      timeout: 5
      success_threshold: 1
      failure_threshold: 3
  
  # Scaling
  scaling:
    enabled: true
    min_replicas: 2
    max_replicas: 5
    target_cpu_utilization: 70
    target_memory_utilization: 80

# Environment Configuration
environment:
  python_version: "3.11"
  dependencies:
    - "pyspark==3.5.0"
    - "tensorflow==2.16.1"
    - "fastapi==0.104.1"
    - "uvicorn==0.24.0"
    - "pydantic==2.5.0"
    - "numpy==1.24.3"
    - "pandas==2.0.3"
    - "scikit-learn==1.3.0"
    - "mlflow==2.9.2"
    - "redis==5.0.1"
  
  # Environment variables
  env_vars:
    SPARK_HOME: "/opt/spark"
    PYSPARK_PYTHON: "/usr/bin/python3.11"
    MLFLOW_TRACKING_URI: "http://mlflow:5000"
    REDIS_URL: "redis://redis:6379/1"
    LOG_LEVEL: "INFO"
    PYTHONPATH: "/app/src:/app/config"

# Error Handling
error_handling:
  # Error responses
  error_responses:
    format: "json"
    include_stack_trace: false
    include_request_id: true
    log_errors: true
  
  # Job error handling
  job_error_handling:
    retry_failed_jobs: true
    max_retries: 3
    retry_delay: "5m"
    dead_letter_queue: true
    error_notification: true
  
  # Graceful shutdown
  graceful_shutdown:
    enabled: true
    timeout: "60s"
    finish_running_jobs: true
    cancel_queued_jobs: true
